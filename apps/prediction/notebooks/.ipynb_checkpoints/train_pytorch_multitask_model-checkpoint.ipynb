{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyTorchマルチタスク学習による競馬予想モデル\n",
        "\n",
        "年度パックNPZファイルからデータを読み込み、PyTorchベースのマルチタスク学習モデルを学習します。\n",
        "\n",
        "## PyTorchマルチタスク学習とは\n",
        "\n",
        "着順予測（ListNet風Listwise Loss）と走破タイム予測（回帰）を**同時に学習**する真のマルチタスク学習です。\n",
        "\n",
        "### アーキテクチャ\n",
        "- **共有エンコーダー**: ResNet風のMLP（BatchNorm + Dropout + スキップ接続）\n",
        "- **着順予測ヘッド**: ランキングスコア出力（ListNet Loss）\n",
        "- **タイム予測ヘッド**: 正規化タイム出力（MSE Loss）\n",
        "\n",
        "### 学習方法\n",
        "- **ListNet Loss**: レース単位で順位確率分布を計算し、クロスエントロピーで損失を計算\n",
        "- **損失関数の重み付け**: 着順予測 0.7、タイム予測 0.3\n",
        "- **同時学習**: 2つのタスクを同時に学習することで、特徴量の共有により精度向上\n",
        "\n",
        "### タスク構成\n",
        "- **タスク1: 着順予測**（ListNet風Listwise Loss）\n",
        "- **タスク2: 走破タイム予測**（回帰）\n",
        "\n",
        "### 事実ベースの特徴量のみを使用\n",
        "- JRDBの事前予想は除外（ペース予想、テン指数、上がり指数、位置指数など）\n",
        "- オッズは除外\n",
        "- 実力に基づいた予測を実現\n",
        "\n",
        "### 期待される効果\n",
        "- より高い的中率（ListNet Lossによる精度向上）\n",
        "- タイム予測による実力評価\n",
        "- マルチタスク学習による特徴量の共有と精度向上\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## インポート\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: /Users/soichiro/Dev/umayomi/apps/prediction/.venv/bin/python\n",
            "PyTorch version: 2.9.0\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "\n",
        "# プロジェクトルート（apps/prediction/）をパスに追加\n",
        "project_root = Path().resolve().parent  # notebooks/ -> apps/prediction/\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# 仮想環境のパスを確認（.venv/bin/python または .venv/Scripts/python）\n",
        "venv_python = project_root / '.venv' / 'bin' / 'python'\n",
        "if not venv_python.exists():\n",
        "    venv_python = project_root / '.venv' / 'Scripts' / 'python.exe'\n",
        "\n",
        "# 仮想環境が存在する場合、sys.executableを確認\n",
        "if venv_python.exists():\n",
        "    current_python = sys.executable\n",
        "    venv_python_str = str(venv_python)\n",
        "    if venv_python_str not in current_python:\n",
        "        print(f\"警告: 仮想環境が有効化されていない可能性があります\")\n",
        "        print(f\"現在のPython: {current_python}\")\n",
        "        print(f\"仮想環境のPython: {venv_python_str}\")\n",
        "        print(f\"仮想環境を有効化するには、以下のコマンドを実行してください:\")\n",
        "        print(f\"  source {project_root}/.venv/bin/activate  # macOS/Linux\")\n",
        "        print(f\"  または\")\n",
        "        print(f\"  {project_root}/.venv/Scripts/activate  # Windows\")\n",
        "\n",
        "# モジュールの再読み込みを確実にする\n",
        "if 'src.preprocessor' in sys.modules:\n",
        "    importlib.reload(sys.modules['src.preprocessor'])\n",
        "if 'src.data_loader' in sys.modules:\n",
        "    importlib.reload(sys.modules['src.data_loader'])\n",
        "if 'src.pytorch_multitask_predictor' in sys.modules:\n",
        "    importlib.reload(sys.modules['src.pytorch_multitask_predictor'])\n",
        "if 'src.features' in sys.modules:\n",
        "    importlib.reload(sys.modules['src.features'])\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "except ImportError as e:\n",
        "    print(f\"エラー: 必要なライブラリがインストールされていません: {e}\")\n",
        "    print(f\"現在のPython: {sys.executable}\")\n",
        "    print(f\"仮想環境のパス: {venv_python if venv_python.exists() else '見つかりません'}\")\n",
        "    print(\"\\n以下のコマンドで仮想環境を有効化してから、パッケージをインストールしてください:\")\n",
        "    print(f\"  cd {project_root}\")\n",
        "    print(f\"  source .venv/bin/activate  # macOS/Linux\")\n",
        "    print(f\"  または\")\n",
        "    print(f\"  .venv\\\\Scripts\\\\activate  # Windows\")\n",
        "    print(f\"  pip install -r requirements.txt\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    from src.data_loader import load_annual_pack_npz, load_multiple_npz_files\n",
        "    from src.preprocessor import Preprocessor\n",
        "    from src.pytorch_multitask_predictor import MultitaskPredictor\n",
        "    from src.features import Features\n",
        "except ImportError as e:\n",
        "    print(f\"エラー: プロジェクトモジュールのインポートに失敗しました: {e}\")\n",
        "    print(f\"プロジェクトルート: {project_root}\")\n",
        "    print(\"プロジェクトルートが正しく設定されているか確認してください。\")\n",
        "    raise\n",
        "\n",
        "# %reload_ext autoreload  # nbconvert実行時はコメントアウト\n",
        "# %autoreload 2\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# PyTorch設定\n",
        "print(f\"Python: {sys.executable}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 設定\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASE_PATH: /Users/soichiro/Dev/umayomi/apps/prediction/notebooks/data\n",
            "見つかったNPZファイル: 10件\n",
            "  - jrdb_npz_SEC_2024.npz\n",
            "  - jrdb_npz_UKC_2024.npz\n",
            "  - jrdb_npz_KYH_2024.npz\n",
            "  - jrdb_npz_KYI_2024.npz\n",
            "  - jrdb_npz_SED_2024.npz\n",
            "見つかったデータタイプ: ['BAC', 'KYI', 'SEC', 'UKC', 'TYB']\n"
          ]
        }
      ],
      "source": [
        "# NPZファイルが格納されているベースパス\n",
        "BASE_PATH = Path('./data')  # apps/prediction/notebooks/data\n",
        "\n",
        "# 使用するデータタイプ\n",
        "# レース開始時点で利用可能なデータのみを含める\n",
        "# オッズデータ（OZ, OW, OU, OT, OV）と払戻データ（HJC, HJB）は除外\n",
        "# 注意: 実際のファイル名は SEC ですが、preprocessor は SED/SEC の両方をサポートしています\n",
        "DATA_TYPES = [\n",
        "    'BAC',  # 番組データ（レース条件・出走馬一覧）\n",
        "    'KYI',  # 競走馬データ（牧場先情報付き・最も詳細）\n",
        "    'SEC',  # 成績速報データ（過去の成績・前走データ抽出に使用、教師データとして使用）\n",
        "    'UKC',  # 馬基本データ（血統登録番号・性別・生年月日・血統情報）\n",
        "    'TYB',  # 直前情報データ（出走直前の馬の状態・当日予想に最重要）\n",
        "]\n",
        "\n",
        "# 使用する年度\n",
        "YEARS = [2024]  # 必要に応じて複数年度を指定\n",
        "\n",
        "# モデル保存パス\n",
        "MODEL_PATH = Path('../models/pytorch_multitask_model_v1.pth')\n",
        "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ファイル名の確認（デバッグ用）\n",
        "print(f\"BASE_PATH: {BASE_PATH.absolute()}\")\n",
        "if BASE_PATH.exists():\n",
        "    npz_files = list(BASE_PATH.glob('*.npz'))\n",
        "    print(f\"見つかったNPZファイル: {len(npz_files)}件\")\n",
        "    if len(npz_files) > 0:\n",
        "        for f in npz_files[:5]:  # 最初の5件を表示\n",
        "            print(f\"  - {f.name}\")\n",
        "        # 必要なデータタイプが存在するか確認\n",
        "        required_prefixes = [f'jrdb_npz_{dt}_' for dt in DATA_TYPES]\n",
        "        found_types = []\n",
        "        for prefix in required_prefixes:\n",
        "            matching = [f for f in npz_files if f.name.startswith(prefix)]\n",
        "            if matching:\n",
        "                found_types.append(prefix.replace('jrdb_npz_', '').replace('_', ''))\n",
        "        print(f\"見つかったデータタイプ: {found_types}\")\n",
        "        missing_types = [dt for dt in DATA_TYPES if dt not in found_types]\n",
        "        if missing_types:\n",
        "            print(f\"警告: 以下のデータタイプが見つかりません: {missing_types}\")\n",
        "    else:\n",
        "        print(\"警告: NPZファイルが見つかりません\")\n",
        "else:\n",
        "    print(f\"警告: {BASE_PATH} が存在しません\")\n",
        "    print(\"データディレクトリを作成するか、正しいパスを指定してください。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## データ読み込みと前処理\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "データ読み込みと前処理を開始します...\n",
            "データ読み込み中...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "データ読み込み:  40%|████      | 2/5 [00:00<00:00, 16.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "読み込み完了: BAC - 合計 3454件\n",
            "読み込み完了: KYI - 合計 47181件\n",
            "読み込み完了: SEC - 合計 51211件\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "データ読み込み: 100%|██████████| 5/5 [00:00<00:00, 17.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "読み込み完了: UKC - 合計 47181件\n",
            "読み込み完了: TYB - 合計 94380件\n",
            "データ結合中...\n",
            "KYIデータ: 総数=47181件\n",
            "BACデータ: 総数=3454件, ユニークレース数=3454件\n",
            "KYIデータ: ユニークレース数=3454件\n",
            "  KYIのみのレース数: 0件\n",
            "  BACのみのレース数: 0件\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/mm/c2gn__y56p58dssh1byn5wbh0000gn/T/ipykernel_30489/346585666.py\", line 6, in <module>\n",
            "    df = preprocessor.process(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soichiro/Dev/umayomi/apps/prediction/src/preprocessor.py\", line 817, in process\n",
            "    combined_df = self.combine_data_types(data_dict)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soichiro/Dev/umayomi/apps/prediction/src/preprocessor.py\", line 284, in combine_data_types\n",
            "    df = FeatureConverter.add_race_key_to_df(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soichiro/Dev/umayomi/apps/prediction/src/feature_converter.py\", line 333, in add_race_key_to_df\n",
            "    year, month, day = FeatureConverter.extract_ymd_from_df_vectorized(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soichiro/Dev/umayomi/apps/prediction/src/feature_converter.py\", line 155, in extract_ymd_from_df_vectorized\n",
            "    ymd_str_primary = df[ymd_col].apply(FeatureConverter.safe_ymd)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soichiro/Dev/umayomi/apps/prediction/.venv/lib/python3.12/site-packages/pandas/core/series.py\", line 4943, in apply\n",
            "    ).apply()\n",
            "      ^^^^^^^\n",
            "  File \"/Users/soichiro/Dev/umayomi/apps/prediction/.venv/lib/python3.12/site-packages/pandas/core/apply.py\", line 1422, in apply\n",
            "    return self.apply_standard()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soichiro/Dev/umayomi/apps/prediction/.venv/lib/python3.12/site-packages/pandas/core/apply.py\", line 1502, in apply_standard\n",
            "    mapped = obj._map_values(\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soichiro/Dev/umayomi/apps/prediction/.venv/lib/python3.12/site-packages/pandas/core/base.py\", line 925, in _map_values\n",
            "    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soichiro/Dev/umayomi/apps/prediction/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py\", line 1743, in map_array\n",
            "    return lib.map_infer(values, mapper, convert=convert)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"pandas/_libs/lib.pyx\", line 2999, in pandas._libs.lib.map_infer\n",
            "  File \"/Users/soichiro/Dev/umayomi/apps/prediction/src/feature_converter.py\", line 53, in safe_ymd\n",
            "    ymd_int = int(value)\n",
            "              ^^^^^^^^^^\n",
            "ValueError: cannot convert float NaN to integer\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "エラーが発生しました: cannot convert float NaN to integer\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "cannot convert float NaN to integer",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mデータ読み込みと前処理を開始します...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     df = \u001b[43mpreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBASE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDATA_TYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43myears\u001b[49m\u001b[43m=\u001b[49m\u001b[43mYEARS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_annual_pack\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mデータ読み込み完了: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m件\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mレース数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.index.nunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/umayomi/apps/prediction/src/preprocessor.py:817\u001b[39m, in \u001b[36mPreprocessor.process\u001b[39m\u001b[34m(self, base_path, data_types, years, use_annual_pack, use_cache, save_cache)\u001b[39m\n\u001b[32m    815\u001b[39m \u001b[38;5;66;03m# 2. データ結合\u001b[39;00m\n\u001b[32m    816\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mデータ結合中...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m combined_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine_data_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[38;5;66;03m# 3. 前走データ抽出\u001b[39;00m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mSED\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data_dict:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/umayomi/apps/prediction/src/preprocessor.py:284\u001b[39m, in \u001b[36mPreprocessor.combine_data_types\u001b[39m\u001b[34m(self, data_dict)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m data_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mSED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSEC\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    281\u001b[39m     \u001b[38;5;66;03m# SED/SECデータも同様に結合（レースキー + 馬番）\u001b[39;00m\n\u001b[32m    282\u001b[39m     \u001b[38;5;66;03m# BACデータから年月日を取得する必要がある場合は、bac_dfを渡す\u001b[39;00m\n\u001b[32m    283\u001b[39m     bac_df_for_sed = data_dict.get(\u001b[33m\"\u001b[39m\u001b[33mBAC\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mBAC\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     df = \u001b[43mFeatureConverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_race_key_to_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbac_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbac_df_for_sed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bac_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbac_df_for_sed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     combined_df = combined_df.merge(\n\u001b[32m    289\u001b[39m         df, on=[\u001b[33m\"\u001b[39m\u001b[33mrace_key\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m馬番\u001b[39m\u001b[33m\"\u001b[39m], how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m, suffixes=(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    290\u001b[39m     )\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m data_type == \u001b[33m\"\u001b[39m\u001b[33mUKC\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# UKCデータは血統登録番号で結合\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/umayomi/apps/prediction/src/feature_converter.py:333\u001b[39m, in \u001b[36mFeatureConverter.add_race_key_to_df\u001b[39m\u001b[34m(df, bac_df, use_bac_date)\u001b[39m\n\u001b[32m    330\u001b[39m df = df.merge(bac_mapping_df, on=\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m, how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m, suffixes=(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_bac\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# 年月日を抽出（ベクトル化）\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m year, month, day = \u001b[43mFeatureConverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_ymd_from_df_vectorized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymd_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mymd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m年\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymd_fallback_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m年月日\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    335\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# 一時カラムを削除\u001b[39;00m\n\u001b[32m    338\u001b[39m df = df.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mymd\u001b[39m\u001b[33m\"\u001b[39m], errors=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/umayomi/apps/prediction/src/feature_converter.py:155\u001b[39m, in \u001b[36mFeatureConverter.extract_ymd_from_df_vectorized\u001b[39m\u001b[34m(df, ymd_col, year_col, ymd_fallback_col)\u001b[39m\n\u001b[32m    152\u001b[39m     mask_fallback = pd.Series(\u001b[38;5;28;01mFalse\u001b[39;00m, index=df.index)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ymd_col \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     ymd_str_primary = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mymd_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFeatureConverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_ymd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m     mask_primary = ymd_str_primary.str.len() == \u001b[32m8\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/umayomi/apps/prediction/.venv/lib/python3.12/site-packages/pandas/core/series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/umayomi/apps/prediction/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/umayomi/apps/prediction/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/umayomi/apps/prediction/.venv/lib/python3.12/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/umayomi/apps/prediction/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/umayomi/apps/prediction/src/feature_converter.py:53\u001b[39m, in \u001b[36mFeatureConverter.safe_ymd\u001b[39m\u001b[34m(value)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 数値の場合は整数に変換してから文字列に\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     ymd_int = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ymd_int).zfill(\u001b[32m8\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# 文字列の場合\u001b[39;00m\n",
            "\u001b[31mValueError\u001b[39m: cannot convert float NaN to integer"
          ]
        }
      ],
      "source": [
        "# 前処理を実行\n",
        "preprocessor = Preprocessor()\n",
        "\n",
        "print(\"データ読み込みと前処理を開始します...\")\n",
        "try:\n",
        "    df = preprocessor.process(\n",
        "        base_path=BASE_PATH,\n",
        "        data_types=DATA_TYPES,\n",
        "        years=YEARS,\n",
        "        use_annual_pack=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\nデータ読み込み完了: {len(df)}件\")\n",
        "    print(f\"レース数: {df.index.nunique()}\")\n",
        "    print(f\"\\nデータ形状: {df.shape}\")\n",
        "    print(f\"\\n列名: {df.columns.tolist()[:20]}...\")  # 最初の20列を表示\n",
        "    \n",
        "    # rankフィールドの確認\n",
        "    if 'rank' in df.columns:\n",
        "        print(f\"\\nrankフィールド: あり（欠損値: {df['rank'].isnull().sum()}件）\")\n",
        "        print(f\"着順の分布: {df['rank'].value_counts().sort_index()}\")\n",
        "    else:\n",
        "        print(\"\\n警告: rankフィールドが見つかりません\")\n",
        "    \n",
        "    # タイムフィールドの確認（SEDデータから取得）\n",
        "    if 'タイム' in df.columns:\n",
        "        print(f\"\\nタイムフィールド: あり（欠損値: {df['タイム'].isnull().sum()}件）\")\n",
        "        print(f\"タイムの基本統計:\")\n",
        "        print(df['タイム'].describe())\n",
        "    else:\n",
        "        print(\"\\n警告: タイムフィールドが見つかりません（SEDデータから取得する必要があります）\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"エラーが発生しました: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## データ分割\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 時系列でデータを分割（未来の情報を使わない）\n",
        "if 'df' not in locals() or df is None:\n",
        "    raise ValueError(\"エラー: データが読み込まれていません。先にCell 6を実行してください。\")\n",
        "\n",
        "if len(df) == 0:\n",
        "    raise ValueError(\"エラー: データが空です。データ読み込みを確認してください。\")\n",
        "\n",
        "# 時系列ソート用のカラムを確認\n",
        "sort_column = None\n",
        "for col in ['start_datetime', '年月日', 'date', 'race_date']:\n",
        "    if col in df.columns:\n",
        "        sort_column = col\n",
        "        break\n",
        "\n",
        "if sort_column is None:\n",
        "    print(\"警告: 時系列カラムが見つかりません。インデックスでソートします。\")\n",
        "    df_sorted = df.copy()\n",
        "    # インデックスがrace_keyの場合、race_keyから日付を抽出を試みる\n",
        "    if df.index.name == 'race_key' or 'race_key' in df.columns:\n",
        "        print(\"race_keyから日付情報を抽出できないため、ランダムに分割します。\")\n",
        "        # ランダムに分割（時系列分割ができない場合）\n",
        "        df_sorted = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    else:\n",
        "        df_sorted = df.copy()\n",
        "else:\n",
        "    print(f\"時系列カラム '{sort_column}' でソートします。\")\n",
        "    df_sorted = df.sort_values(sort_column, ascending=True)\n",
        "\n",
        "# 学習データ: 80%\n",
        "# 検証データ: 20%\n",
        "split_idx = int(len(df_sorted) * 0.8)\n",
        "\n",
        "if split_idx == 0:\n",
        "    raise ValueError(\"エラー: データが少なすぎます（分割後0件）。\")\n",
        "\n",
        "train_df = df_sorted.iloc[:split_idx].copy()\n",
        "val_df = df_sorted.iloc[split_idx:].copy()\n",
        "\n",
        "print(f\"学習データ: {len(train_df)}件 ({len(train_df) / len(df_sorted) * 100:.1f}%)\")\n",
        "print(f\"検証データ: {len(val_df)}件 ({len(val_df) / len(df_sorted) * 100:.1f}%)\")\n",
        "\n",
        "if sort_column:\n",
        "    print(f\"\\n学習データの期間: {train_df[sort_column].min()} ～ {train_df[sort_column].max()}\")\n",
        "    print(f\"検証データの期間: {val_df[sort_column].min()} ～ {val_df[sort_column].max()}\")\n",
        "\n",
        "# rankとタイムの確認\n",
        "if 'rank' in train_df.columns:\n",
        "    train_rank_count = train_df['rank'].notna().sum()\n",
        "    print(f\"\\n学習データのrank有効数: {train_rank_count} / {len(train_df)}\")\n",
        "else:\n",
        "    print(\"\\n警告: 学習データに'rank'カラムがありません\")\n",
        "\n",
        "if 'タイム' in train_df.columns:\n",
        "    train_time_count = train_df['タイム'].notna().sum()\n",
        "    print(f\"学習データのタイム有効数: {train_time_count} / {len(train_df)}\")\n",
        "else:\n",
        "    print(\"警告: 学習データに'タイム'カラムがありません\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorchマルチタスク学習\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# マルチタスク学習モデルを初期化\n",
        "if 'train_df' not in locals() or train_df is None:\n",
        "    raise ValueError(\"エラー: 学習データが準備されていません。先にCell 8を実行してください。\")\n",
        "if 'val_df' not in locals() or val_df is None:\n",
        "    raise ValueError(\"エラー: 検証データが準備されていません。先にCell 8を実行してください。\")\n",
        "\n",
        "# データの確認\n",
        "if len(train_df) == 0:\n",
        "    raise ValueError(\"エラー: 学習データが空です。\")\n",
        "if len(val_df) == 0:\n",
        "    raise ValueError(\"エラー: 検証データが空です。\")\n",
        "\n",
        "# rankとタイムの有効データ数を確認\n",
        "train_valid = train_df.dropna(subset=['rank', 'タイム']) if 'rank' in train_df.columns and 'タイム' in train_df.columns else train_df\n",
        "val_valid = val_df.dropna(subset=['rank', 'タイム']) if 'rank' in val_df.columns and 'タイム' in val_df.columns else val_df\n",
        "\n",
        "print(f\"学習データ: {len(train_df)}件（有効データ: {len(train_valid)}件）\")\n",
        "print(f\"検証データ: {len(val_df)}件（有効データ: {len(val_valid)}件）\")\n",
        "\n",
        "if len(train_valid) == 0:\n",
        "    print(\"警告: 学習データに有効なrank/タイムデータがありません。\")\n",
        "if len(val_valid) == 0:\n",
        "    print(\"警告: 検証データに有効なrank/タイムデータがありません。\")\n",
        "\n",
        "try:\n",
        "    predictor = MultitaskPredictor(\n",
        "        train_df=train_df,\n",
        "        val_df=val_df,\n",
        "        hidden_dims=[512, 256, 128, 64],  # ResNet風MLPの隠れ層サイズ\n",
        "        dropout=0.3,  # 過学習防止\n",
        "        rank_weight=0.7,  # 着順予測の重み\n",
        "        time_weight=0.3,  # タイム予測の重み\n",
        "        learning_rate=1e-3,\n",
        "        device=None  # 自動選択（CUDAがあれば使用）\n",
        "    )\n",
        "\n",
        "    print(f\"\\nモデルアーキテクチャ:\")\n",
        "    print(f\"  入力次元: {len(predictor.train_dataset.feature_names)}\")\n",
        "    print(f\"  隠れ層: {[512, 256, 128, 64]}\")\n",
        "    print(f\"  デバイス: {predictor.device}\")\n",
        "    print(f\"\\n学習データセット: {len(predictor.train_dataset)}レース\")\n",
        "    print(f\"検証データセット: {len(predictor.val_dataset)}レース\")\n",
        "except Exception as e:\n",
        "    print(f\"エラーが発生しました: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\nデバッグ情報:\")\n",
        "    print(f\"  train_df.shape: {train_df.shape}\")\n",
        "    print(f\"  train_df.columns: {train_df.columns.tolist()[:10]}...\")\n",
        "    if 'rank' in train_df.columns:\n",
        "        print(f\"  rank有効数: {train_df['rank'].notna().sum()}\")\n",
        "    if 'タイム' in train_df.columns:\n",
        "        print(f\"  タイム有効数: {train_df['タイム'].notna().sum()}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 学習を実行\n",
        "if 'predictor' not in locals() or predictor is None:\n",
        "    raise ValueError(\"エラー: モデルが初期化されていません。先にCell 10を実行してください。\")\n",
        "if 'MODEL_PATH' not in locals():\n",
        "    raise ValueError(\"エラー: MODEL_PATHが定義されていません。先にCell 4を実行してください。\")\n",
        "\n",
        "# データセットの確認\n",
        "if len(predictor.train_dataset) == 0:\n",
        "    raise ValueError(\"エラー: 学習データセットが空です。\")\n",
        "if len(predictor.val_dataset) == 0:\n",
        "    print(\"警告: 検証データセットが空です。検証スキップモードで学習します。\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PyTorchマルチタスク学習を開始\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"学習レース数: {len(predictor.train_dataset)}\")\n",
        "print(f\"検証レース数: {len(predictor.val_dataset)}\")\n",
        "print(f\"デバイス: {predictor.device}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    history = predictor.train(\n",
        "        num_epochs=50,\n",
        "        early_stopping_patience=10,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n学習完了\")\n",
        "    print(f\"モデル保存パス: {MODEL_PATH}\")\n",
        "\n",
        "    # モデルを保存\n",
        "    predictor.save_model(str(MODEL_PATH))\n",
        "    print(f\"モデルを保存しました: {MODEL_PATH}\")\n",
        "    \n",
        "    # 学習履歴の確認\n",
        "    if history and len(history.get('total_loss', [])) > 0:\n",
        "        print(f\"\\n最終損失: {history['total_loss'][-1]:.4f}\")\n",
        "        if len(history.get('val_ndcg', [])) > 0:\n",
        "            print(f\"最終NDCG@3: {history['val_ndcg'][-1]:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"エラーが発生しました: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\nデバッグ情報:\")\n",
        "    print(f\"  学習データセットサイズ: {len(predictor.train_dataset)}\")\n",
        "    print(f\"  検証データセットサイズ: {len(predictor.val_dataset)}\")\n",
        "    print(f\"  デバイス: {predictor.device}\")\n",
        "    if len(predictor.train_dataset) > 0:\n",
        "        try:\n",
        "            # 最初のサンプルを取得して確認\n",
        "            sample = predictor.train_dataset[0]\n",
        "            print(f\"  サンプル特徴量形状: {sample['features'].shape}\")\n",
        "            print(f\"  サンプルrank_targets形状: {sample['rank_targets'].shape}\")\n",
        "            print(f\"  サンプルtime_targets形状: {sample['time_targets'].shape}\")\n",
        "        except Exception as sample_e:\n",
        "            print(f\"  サンプル取得エラー: {sample_e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 学習結果の可視化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 学習履歴を可視化\n",
        "if 'history' not in locals() or history is None:\n",
        "    raise ValueError(\"エラー: 学習履歴がありません。先にCell 11を実行してください。\")\n",
        "\n",
        "# 履歴が空でないか確認\n",
        "if not isinstance(history, dict) or len(history) == 0:\n",
        "    print(\"警告: 学習履歴が空です。学習が正常に完了していない可能性があります。\")\n",
        "else:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # 損失の推移\n",
        "    has_loss_data = False\n",
        "    if len(history.get('total_loss', [])) > 0:\n",
        "        axes[0, 0].plot(history['total_loss'], label='Total Loss')\n",
        "        has_loss_data = True\n",
        "    if len(history.get('rank_loss', [])) > 0:\n",
        "        axes[0, 0].plot(history['rank_loss'], label='Rank Loss')\n",
        "        has_loss_data = True\n",
        "    if len(history.get('time_loss', [])) > 0:\n",
        "        axes[0, 0].plot(history['time_loss'], label='Time Loss')\n",
        "        has_loss_data = True\n",
        "    \n",
        "    if has_loss_data:\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True)\n",
        "    else:\n",
        "        axes[0, 0].text(0.5, 0.5, 'No loss data', ha='center', va='center')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Training Loss')\n",
        "\n",
        "    # 検証NDCGの推移\n",
        "    if len(history.get('val_ndcg', [])) > 0:\n",
        "        axes[0, 1].plot(history['val_ndcg'], label='Validation NDCG@3', color='green')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True)\n",
        "    else:\n",
        "        axes[0, 1].text(0.5, 0.5, 'No validation data', ha='center', va='center')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('NDCG@3')\n",
        "    axes[0, 1].set_title('Validation NDCG@3')\n",
        "\n",
        "    # 最終エポックの損失内訳\n",
        "    if len(history.get('rank_loss', [])) > 0 and len(history.get('time_loss', [])) > 0:\n",
        "        axes[1, 0].bar(['Rank Loss', 'Time Loss'], \n",
        "                       [history['rank_loss'][-1], history['time_loss'][-1]])\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        axes[1, 0].set_title('Final Loss Breakdown')\n",
        "    else:\n",
        "        axes[1, 0].text(0.5, 0.5, 'No loss data', ha='center', va='center')\n",
        "        axes[1, 0].set_title('Final Loss Breakdown')\n",
        "\n",
        "    # ベストNDCG\n",
        "    if len(history.get('val_ndcg', [])) > 0:\n",
        "        best_ndcg = max(history['val_ndcg'])\n",
        "        best_epoch = history['val_ndcg'].index(best_ndcg) + 1\n",
        "        axes[1, 1].text(0.5, 0.5, f'Best NDCG@3: {best_ndcg:.4f}\\nBest Epoch: {best_epoch}',\n",
        "                        ha='center', va='center', fontsize=14, \n",
        "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    else:\n",
        "        axes[1, 1].text(0.5, 0.5, 'No validation data',\n",
        "                        ha='center', va='center', fontsize=14, \n",
        "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    axes[1, 1].axis('off')\n",
        "    axes[1, 1].set_title('Best Performance')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n最終結果:\")\n",
        "    if len(history.get('val_ndcg', [])) > 0:\n",
        "        best_ndcg = max(history['val_ndcg'])\n",
        "        best_epoch = history['val_ndcg'].index(best_ndcg) + 1\n",
        "        print(f\"  最終NDCG@3: {history['val_ndcg'][-1]:.4f}\")\n",
        "        print(f\"  ベストNDCG@3: {best_ndcg:.4f} (Epoch {best_epoch})\")\n",
        "    else:\n",
        "        print(f\"  最終NDCG@3: N/A（検証データなし）\")\n",
        "        print(f\"  ベストNDCG@3: N/A\")\n",
        "    if len(history.get('rank_loss', [])) > 0:\n",
        "        print(f\"  最終Rank Loss: {history['rank_loss'][-1]:.4f}\")\n",
        "    else:\n",
        "        print(f\"  最終Rank Loss: N/A\")\n",
        "    if len(history.get('time_loss', [])) > 0:\n",
        "        print(f\"  最終Time Loss: {history['time_loss'][-1]:.4f}\")\n",
        "    else:\n",
        "        print(f\"  最終Time Loss: N/A\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 予測と評価\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 検証データで予測\n",
        "if 'predictor' not in locals() or predictor is None:\n",
        "    raise ValueError(\"エラー: モデルが初期化されていません。先にCell 10を実行してください。\")\n",
        "if 'val_df' not in locals() or val_df is None:\n",
        "    raise ValueError(\"エラー: 検証データが準備されていません。先にCell 8を実行してください。\")\n",
        "\n",
        "print(\"検証データで予測を実行...\")\n",
        "try:\n",
        "    predictions = predictor.predict(val_df)\n",
        "    print(f\"予測完了: {len(predictions)}件の予測結果\")\n",
        "except Exception as e:\n",
        "    print(f\"予測中にエラーが発生しました: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# インデックスがrace_keyかどうかを確認\n",
        "if predictions.index.name == 'race_key':\n",
        "    race_keys = predictions.index.unique()\n",
        "    print(f\"race_keyがインデックス: {len(race_keys)}レース\")\n",
        "else:\n",
        "    # race_keyがカラムの場合\n",
        "    if 'race_key' in predictions.columns:\n",
        "        race_keys = predictions['race_key'].unique()\n",
        "        print(f\"race_keyがカラム: {len(race_keys)}レース\")\n",
        "    else:\n",
        "        # race_keyが見つからない場合、インデックスから取得を試みる\n",
        "        if hasattr(predictions.index, 'names') and 'race_key' in predictions.index.names:\n",
        "            race_keys = predictions.index.get_level_values('race_key').unique()\n",
        "            print(f\"race_keyがマルチインデックス: {len(race_keys)}レース\")\n",
        "        else:\n",
        "            raise ValueError(\"race_keyが見つかりません（インデックスにもカラムにも存在しません）\")\n",
        "\n",
        "# 予測結果を確認\n",
        "print(f\"\\n予測結果のサンプル（最初の5レース）:\")\n",
        "sample_races = list(race_keys)[:5] if len(race_keys) > 0 else []\n",
        "\n",
        "if len(sample_races) == 0:\n",
        "    print(\"警告: 予測対象のレースがありません\")\n",
        "else:\n",
        "    for race_key in sample_races:\n",
        "        try:\n",
        "            # レースデータを取得\n",
        "            if predictions.index.name == 'race_key':\n",
        "                race_pred = predictions.loc[race_key].copy()\n",
        "            elif hasattr(predictions.index, 'names') and 'race_key' in predictions.index.names:\n",
        "                race_pred = predictions.loc[predictions.index.get_level_values('race_key') == race_key].copy()\n",
        "            else:\n",
        "                race_pred = predictions[predictions['race_key'] == race_key].copy()\n",
        "            \n",
        "            if len(race_pred) == 0:\n",
        "                print(f\"\\nレース: {race_key} - データが見つかりません\")\n",
        "                continue\n",
        "            \n",
        "            # rank_predでソート（降順：スコアが高い順）\n",
        "            if 'rank_pred' in race_pred.columns:\n",
        "                race_pred = race_pred.sort_values('rank_pred', ascending=False)\n",
        "            \n",
        "            print(f\"\\nレース: {race_key} ({len(race_pred)}頭)\")\n",
        "            \n",
        "            # 表示するカラムを選択\n",
        "            display_cols = []\n",
        "            if 'rank' in race_pred.columns:\n",
        "                display_cols.append('rank')\n",
        "            if 'rank_pred' in race_pred.columns:\n",
        "                display_cols.append('rank_pred')\n",
        "            if 'time_pred' in race_pred.columns:\n",
        "                display_cols.append('time_pred')\n",
        "            if 'タイム' in race_pred.columns:\n",
        "                display_cols.append('タイム')\n",
        "            \n",
        "            if display_cols:\n",
        "                print(race_pred[display_cols].head(10))\n",
        "            else:\n",
        "                print(\"表示可能なカラムが見つかりません\")\n",
        "                print(f\"利用可能なカラム: {race_pred.columns.tolist()[:10]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nレース: {race_key} - エラー: {e}\")\n",
        "            continue\n",
        "\n",
        "# NDCGを計算（レース単位）\n",
        "from src.evaluator import calculate_ndcg\n",
        "\n",
        "ndcg_scores = []\n",
        "valid_races = 0\n",
        "skipped_races = 0\n",
        "\n",
        "for race_key in race_keys:\n",
        "    try:\n",
        "        # レースデータを取得\n",
        "        if predictions.index.name == 'race_key':\n",
        "            race_pred = predictions.loc[race_key].copy()\n",
        "        elif hasattr(predictions.index, 'names') and 'race_key' in predictions.index.names:\n",
        "            race_pred = predictions.loc[predictions.index.get_level_values('race_key') == race_key].copy()\n",
        "        else:\n",
        "            race_pred = predictions[predictions['race_key'] == race_key].copy()\n",
        "        \n",
        "        # rankとrank_predが存在し、NaNでないデータのみを使用\n",
        "        if 'rank' not in race_pred.columns or 'rank_pred' not in race_pred.columns:\n",
        "            skipped_races += 1\n",
        "            continue\n",
        "        \n",
        "        # NaNを除外\n",
        "        race_pred_clean = race_pred.dropna(subset=['rank', 'rank_pred'])\n",
        "        \n",
        "        if len(race_pred_clean) == 0:\n",
        "            skipped_races += 1\n",
        "            continue\n",
        "        \n",
        "        race_rank_preds = race_pred_clean['rank_pred'].values\n",
        "        race_rank_targets = race_pred_clean['rank'].values\n",
        "        \n",
        "        # NaNチェック\n",
        "        if np.isnan(race_rank_preds).any() or np.isnan(race_rank_targets).any():\n",
        "            skipped_races += 1\n",
        "            continue\n",
        "        \n",
        "        ndcg = calculate_ndcg(race_rank_targets, race_rank_preds, k=3)\n",
        "        ndcg_scores.append(ndcg)\n",
        "        valid_races += 1\n",
        "    except Exception as e:\n",
        "        skipped_races += 1\n",
        "        continue\n",
        "\n",
        "if len(ndcg_scores) > 0:\n",
        "    print(f\"\\n検証データ全体のNDCG@3: {np.mean(ndcg_scores):.4f}\")\n",
        "    print(f\"NDCG@3の標準偏差: {np.std(ndcg_scores):.4f}\")\n",
        "    print(f\"評価対象レース数: {valid_races} / {len(race_keys)}\")\n",
        "    if skipped_races > 0:\n",
        "        print(f\"スキップされたレース数: {skipped_races}\")\n",
        "else:\n",
        "    print(\"\\n警告: 評価可能なレースがありません（rankまたはrank_predが欠損）\")\n",
        "    print(f\"総レース数: {len(race_keys)}\")\n",
        "    print(f\"スキップされたレース数: {skipped_races}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (prediction)",
      "language": "python",
      "name": "prediction"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
